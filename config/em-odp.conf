# EM runtime configuration options
#
# This template configuration file (em-odp.conf) is hardcoded
# during the configure/build phase into em-odp and the values defined here are
# used at runtime unless overridden by the user with the optional environment
# variable EM_CONFIG_FILE=my-emodp.conf at program startup.
#
# This configuration file MUST include all configuration options.
#
# The environment variable EM_CONFIG_FILE can be used to override default values
# and it doesn't have to include all available options. The missing options are
# replaced with hardcoded default values.
#
# The options defined here are implementation specific and valid option
# values should be checked from the implementation code.

# Mandatory fields
em_implementation = "em-odp"
config_file_version = "0.0.19"

# Pool options
pool: {
	# Enable EM pool usage statistics collection during runtime (true/false).
	#
	# Pool statistics is updated e.g. during em_alloc() and em_free(). They
	# can be obtained by calling em_pool_stats(), em_pool_subpool_stats()
	# and em_pool_info(), which returns also pool usage statistics if
	# 'statistics.available' or 'statistics.cache_available' is set to true.
	#
	# These are global settings concerning all pools. A similar, but
	# pool-specific option, is 'em_pool_cfg_t::stats_opt{}', that overrides
	# these global settings for a specific pool when given to
	# em_pool_create(..., pool_cfg).
	#
	# Note that following settings are ineffective without ODP support, see
	# ODP capabilities.
	statistics: {
		# Number of available events in the pool.
		available = false

		# Number of alloc operations from the pool.
		# Includes both successful and failed operations (e.g. when pool
		# is empty).
		alloc_ops = false

		# Number of failed alloc operations (pool empty).
		alloc_fails = false

		# Number of free operations to the pool.
		free_ops = false

		# Total number of alloc and free operations.
		# Includes both successful and failed operations (pool empty).
		total_ops = false

		# Number of available events in the local caches of all cores.
		cache_available = false

		# Number of successful alloc operations from pool caches (returned
		# at least one event).
		cache_alloc_ops = false

		# Number of free operations, which stored events to pool caches.
		cache_free_ops = false

		# Number of available events in each core local cache.
		# Note that EM APIs(e.g. em_pool_stats() and em_pool_subpool_stats())
		# do not support fetching core_cache_available. Instead, this setting
		# only enables applications to use ODP API (e.g odp_pool_stats()) to
		# read odp_pool_stats_opt_t::thread_cache_available.
		core_cache_available = false
	}

	# Default alignment offset in bytes for the event payload start address
	#
	# Set the event payload alignment offset for events allocated from
	# any pool. This is a global setting concerning all pools.
	# A similar, but pool-specific option, is 'em_pool_cfg_t::align_offset{}'
	# that overrides this global setting for a specific pool when given to
	# em_pool_create(..., pool_cfg).
	#
	# Use this option to globally adjust the payload layout so that a
	# specific part of it can be placed at a needed alignment for e.g.
	# HW access.
	#
	# The default EM event payload start address alignment is a power-of-two
	# that is at minimum 32 bytes (i.e. 32 B, 64 B, 128 B etc. depending on
	# e.g. target cache-line size).
	# The 'align_offset' option can be used to fine-tune the start-address
	# by a small offset to e.g. make room for a small SW header before the
	# rest of the payload that might need a specific alignment for direct
	# HW-access.
	# Example: setting 'align_offset = 8' makes sure that the payload
	# _after_ 8 bytes will be aligned at minimum (2^x) 32 bytes for all
	# pools that do not override this value.
	#
	#   start: base - align_offset
	#        |
	#        v
	#        +------------------------------------------+
	#        | <----- |  Event Payload                  |
	#        +------------------------------------------+
	#                 ^
	#                 |
	#                base (min 32B aligned, power-of-two)
	align_offset = 0

	# Default event user area size in bytes for all events from all pools.
	#
	# The event user area is located within the event metadata (hdr) and
	# is not part of the event payload. The event user area can e.g. be
	# used to store additional state data related to the event payload
	# content.
	#
	# This is a global setting that can be overridden per pool using
	# 'em_pool_cfg_t::user_area{}' with em_pool_create().
	#
	user_area_size = 0

 	# Default minimum packet headroom in bytes for events allocated from
	# EM-pools of type: EM_EVENT_TYPE_PACKET. Ignored for other pool types.
	#
	# This is a global setting for EM-pools of type EM_EVENT_TYPE_PACKET.
	# A similar, but pool-specific option, is 'em_pool_cfg_t::pkt.headroom{}'
	# that overrides this global setting for a specific pkt-pool when given
	# to em_pool_create(..., pool_cfg).
	#
	# Use this option to globally set the minimum headroom in bytes for
	# events/packets allocated from pkt-pools. Each event/packet will have
	# at least this much headroom.
	#
	# 0: Explicitly set 'No headroom' for the pool.
	#
	# Max value is determined by the ODP implementation capabilities
	# (a value larger than 'max' will lead to setup error)
	#
	# Note: using 'align_offset > 0' reduces the packet-headroom by the
	#       same amount, for example:
	#       "align_offset=8, pkt_headroom=128 ==> headroom= 128-8 = 120 B"
	#
	pkt_headroom = 128
}

queue_group: {
	# Create the EM single-core queue groups (true/false)
	#
	# Select whether EM should create a queue group per EM-core.
	# Corresponds to the queue groups with name:
	# EM_QUEUE_GROUP_CORE_BASE_NAME + "%d"
	# Each created queue group only has one core set in its core mask.
	#
	# EM earlier relied on these queue groups for internal core specific
	# messaging and also allowed applicatioins to use them. Currently EM
	# does not internally need these groups but will create them based on
	# this option for applications relying on their existence.
	create_core_queue_groups = false
}

# Queue options
queue: {
	# Default minimum number of events that a queue can hold.
	#
	# This value will be used in queue creation (em_queue_create*()) if no
	# other size information has been provided via the
	# em_queue_conf_t::min_events parameter.
	# Setting 'min_events_default = 0' will use the odp-implementation's
	# default values (might vary from one odp-implementation to another).
	min_events_default = 4095

	priority: {
		# Select the queue priority mapping mode (EM API to ODP)
		#
		#  0: legacy, map EM prios to ODP min/default/max, 3 prios only
		#     (old default used before adding the map_mode option)
		#  1: map according to ODP runtime number of priorities
		#     (linear fit to full range of ODP priorities available)
		#  2: custom priority mapping
		#     (use custom_map below)
		#
		map_mode = 1

		# Custom priority map (required when map_mode = 2)
		#
		# This array needs to have EM_QUEUE_PRIO_NUM entries
		# (typically 8). First entry is for lowest priority (0).
		# The value is ADDED to odp_schedule_min_prio() and then passed
		# to ODP, i.e. values are offsets from odp_schedule_min_prio().
		# Values given here must be valid for ODP runtime configuration,
		# i.e. value plus odp_schedule_min_prio() must not exceed
		# odp_schedule_max_prio().
		#
		#custom_map = [0, 0, 1, 3 ,4 ,6 ,7, 7]
	}
}

# Event-Chaining options
event_chaining: {
	# Note:
	# The user _must_ provide an implementation for the overrideable
	# 'event_send_device()' and 'event_send_device_multi()' functions in
	# order to use Event-Chaining functionality!
	# The functions are declared with '__attribute__((weak))' in the EM-lib
	# to allow overriding.
	# Linking user code, which includes overrides for those functions,
	# against the EM-lib will replace the stubs with the user provided
	# variants.

	# Order calls to 'event_send_device()' and 'event_send_device_multi()'
	# when events are sent during an ordered context - true/false.
	#
	# Note: Only meaningful when used together with (parallel-)ordered
	#       queues. Order is implicitly guaranteed when sent during an
	#       atomic context and never guaranteed when sent during a parallel
	#       context.
	# Only set to 'true' if queues of type EM_QUEUE_TYPE_PARALLEL_ORDERED
	# are used with event chaining, no benefit otherwise.
	#
	# Events sent with em_send...() to an event chaining queue will be
	# passed to the user provided 'event_send_device/_multi()' functions.
	# EM can ensure that calls to these functions are:
	# 1) made in order (order_keep = true)
	#    or
	# 2) make no extra effort to keep the order (order_keep = false),
	#    e.g. if the user (or packet output) handles ordering or does not
	#    care about it.
	order_keep = false

	# Number of queues used for ordering event-chaining events
	#
	# Note: Ignored if 'order_keep = false' above.
	#
	# Maintain the order of events sent to an another device from within
	# an EM ordered context (i.e. send from within the EO receive function
	# when handling an event from a parallel-ordered queue).
	# An event-chaining queue has no 'real' queue associated with it, the
	# queue-id simply indicates that events sent to the queue should be
	# sent out of EM via a user specified function (and perhaps out towards
	# another device). Sending events out of EM requires some intervention,
	# especially sending during an ordered context needs to maintain the
	# event order as determined by the context. To maintain event order for
	# event-chaining queues, em-odp uses a set of 'real' queues for order
	# tracking - the number of these queues is set by the following option:
	# (0 = no ordering)
	num_order_queues = 8
}

# Event State Verification (ESV) options
#
# Note: Options only considered if ESV is enabled at compile-time
#       via the configure option '--enable-esv', see configure.ac
esv: {
	# Runtime ESV enable/disable option.
	# Allows the user to disable ESV without recompilation when
	# 'configure --enable-esv'.
	enable = true

	# Store the valid state for the event at runtime each time the internal
	# event state changes so that ESV errors can log both the previous
	# valid state ('prev-state') and the last erroneous state ('new-state')
	# when logging the error:
	#
	# EM Error log (send after free error):
	# 1) esv.store_state = true:
	#    EM ERROR:0x80000010 ESCOPE:0xFF000201 EO:0x2-"eo1"
	#    ESV: Event:0x2007fa6822300 state error ...
	#      prev-state:em_free() core:01: EO:0x2-"eo1" Q:0x3-"Q1" ...
	#    => new-state:em_send() core:00: EO:0x2-"eo1" Q:0x3-"Q1" ...
	#
	# 2) esv.store_state = false (note "prev-state:n/a"):
	#    EM ERROR:0x80000010 ESCOPE:0xFF000201 EO:0x2-"eo1"
	#    ESV: Event:0x2007fa6822300 state error ...
	#      prev-state:n/a
	#    => new-state:em_send() core:00: EO:0x2-"eo1" Q:0x3-"Q1" ...
	#
	# Disabling (= false) might improve runtime performance at the cost of
	# losing debug data when an ESV error occurs.
	#
	# Note: The state will not be stored for event references since they
	# share the same state-data and would overwrite each other's state.
	#
	store_state = true

	# Store the first 32bits of the event payload during each valid
	# event-state transition. This allows for a comparison of the payload
	# content between the previous valid state and the invalid state.
	# Note: The first 32bits of the payload will always be printed in the
	#       error log for the invalid state regardless of this setting.
	# Enabling will impact performace somewhat.
	store_payload_first_u32 = false

	# Preallocate all events in a pool during pool creation to set
	# an initial ESV-state for each event that can be tracked over
	# multiple allocs and frees.
	prealloc_pools = true
}

# EM Command Line Interface options
#
# Example usage:
# Access EM CLI server with telnet client
#	$ telnet
#	telnet> open 127.0.0.1 55555 # or localhost
#	EM-ODP> help
#	EM-ODP> em_core_print
cli: {
	# Runtime cli enable/disable option.
	# By default cli is disabled. To enable cli, set this to true.
	enable = false

	# IP address to which the EM CLI server will be bound to.
	#
	# The IP address can be set to one of the following three types:
	#
	# 	1. "127.0.0.1": will receive data only from the same machine.
	#
	# 	2. Any one of the IP addresses of your machine. Command ifconfig
	#	   can be used to get the IP addresses of your machine.
	#
	#	3. "0.0.0.0": will receive data sent to any IP address of your
	#	   machine.
	#
	# The last two types of IP address can receive data from any other
	# machine in the Internet. Note that this might introduce security
	# concerns.
	#
	ip_addr = "127.0.0.1" # localhost

	# TCP port for the CLI server to receive data.
	port = 55555
}

dispatch: {
	# Poll interval for EM control events (in dispatch rounds)
	#
	# Rate limit EM control queue polling:
	# Poll the EM internal unscheduled control queues for events every
	# N:th dispatch round (where N is 'poll_ctrl_interval' here).
	# The events polled for are related to EM API create/delete/sync calls
	# etc. that need internal communication between the cores.
	#
	# 1) If 'poll_ctrl_interval = 1': polling is done every dispatch round.
	# 2) If 'poll_ctrl_interval = N (>1)': Every N:th dispatch round check
	#    whether 'poll_ctrl_interval_ns' nanoseconds has passed since the
	#    previous poll, and if it has passed then poll the ctrl queues.
	poll_ctrl_interval = 100 # check need to poll every Nth dispatch round

	# Poll interval for EM control events (in nanoseconds)
	#
	# Works together with 'poll_ctrl_interval' to limit polling:
	# When 'poll_ctrl_interval' is larger than 1, use this option to limit
	# the polling rate more exactly in nanoseconds.
	# The previous option 'poll_ctrl_interval' is intended to limit the need
	# to check absolute time(in ns) and thus maybe save some on performance,
	# while this option serves to give more control over the polling rate in
	# nanoseconds.
	poll_ctrl_interval_ns = 1000000L # poll max every 1ms

	# Core local interval for calling input poll and output drain functions
	# (in dispatch rounds)
	#
	# Rate limit EM poll and drain functions:
	# Call the poll and/or drain functions for events every
	# N:th dispatch round on a core (where N is 'poll_drain_interval' here).
	#
	# 1) If 'poll_drain_interval = 1': polling and draining is done every
	#    dispatch round.
	# 2) If 'poll_drain_interval = N (>1)': Every N:th dispatch round check
	#    whether 'poll_drain_interval_ns' nanoseconds has passed since the
	#    previous poll/drain, and if it has passed then call the poll/drain
	#    functions.
	poll_drain_interval = 1

	# Core local interval for calling input poll and output drain functions
	# (in nanoseconds)
	#
	# Works together with 'poll_drain_interval' to limit polling:
	# When 'poll_drain_interval' is larger than 1, use this option to limit
	# the poll/drain rate more exactly in nanoseconds per core.
	# The previous option 'poll_drain_interval' is intended to limit the need
	# to check absolute time(in ns) and thus maybe save some on performance,
	# while this option serves to give more control over the poll/drain rate
	# in nanoseconds.
	poll_drain_interval_ns = 1000000L

	# Runtime schedule-wait option in nanoseconds for em_dispatch() calls.
	# Allows the user to configure the time (ns) the scheduler should wait
	# for events before returning (when no events are immediately available).
	# Only usable if the define EM_SCHED_WAIT_ENABLE has been set to '1'
	# either via
	# 1) 'configure --enable-sched-wait' or
	# 2) #define EM_SCHED_WAIT_ENABLE  0 ==> 1
	#    (include/event_machine/platform/event_machine_config.h)
	# Note that using option 1), i.e. via 'configure', overrides the value
	# in event_machine_config.h.
	# See configure.ac, config/em-odp.conf and event_machine_config.h
	#
	# sched_wait_ns = 0:  no waiting for events, scheduler returns
	#                     immediately even if no events available
	#                     (converted to 'ODP_SCHED_NO_WAIT')
	# sched_wait_ns = -1: waits indefinitly for events
	#                     (converted to 'ODP_SCHED_WAIT')
	# sched_wait_ns > 0:  waits 'sched_wait_ns' for events
	#                     (converted to 'odp_schedule_wait_time(sched_wait_ns)')
	# Note that the 'sched_wait_ns' may be rounded up as specified by the
	# ODP spec, configuration and implementation.
	# Only affects the em_dispatch() function.
	sched_wait_ns = 10000L

	# Pause scheduling for an EM-core returning from em_dispatch().
	# The core is paused from participation in global scheduling and EM
	# empties all locally pre-scheduled events, as well as dispatches them,
	# before returning from the em_dispatch() function.
	# The scheduler should be paused on a core that is leaving the
	# dispatch / scheduling loop for a longer time to avoid locking atomic
	# or ordered contexts (thus blocking other cores from handling events
	# from those same contexts / queues).
	# Only affects the em_dispatch() function.
	sched_pause = false
}

timer: {
	# Use shared tmo handle pool instead of legacy per timer pool.
	shared_tmo_pool_enable = false

	# Defines the shared pool size if shared_tmo_pool_enable = true,
	# i.e. total of allocated timeouts for all timers.
	# Each timer reserves the given number of maximum timeouts at create
	# from this global amount. If not enough is left the timer creation
	# will fail. Value is number of tmo handles (buffers).
	shared_tmo_pool_size = 2000

	# Local cache size for tmo pool(s). Applies to both shared pool and
	# per timer pools
	tmo_pool_cache = 16

	# Configuration of periodic ring timers
	ring: {
		# Periodic ring timeout indication event pool size
		#
		# This is shared pool for all ring timers e.g. size is total.
		# Pool is created when first ring timer is created.
		# Pool contains pre-defined EM_EVENT_TYPE_TIMER_IND events used
		# as timeout indication. Value is number of events.
		timer_event_pool_size = 1000

		# periodic ring timeout event pool local cache size
		timer_event_pool_cache = 16
	}
}

# Configure startup pool(s). Optional. When set, EM will create startup pool(s)
# according to the configuration given here during em_init(). If not given, only
# the default pool will be created. In this case, the default pool configuration
# is specified in the parameters of em_init(), more specifically, in the struct
# em_conf_t::default_pool_cfg.
#
# Note that if the default pool is configured here (e.g. either by setting
# startup_pools.conf[i].name to EM_POOL_DEFAULT_NAME ("default"), by setting
# startup_pools.conf[i].pool to EM_POOL_DEFAULT (1), or by setting both), it
# overrides the default pool configuration given in the parameter of
# em_init(em_conf_t conf). The default pool name or pool ID can not be combined
# with non-default pool IDs and names (e.g. setting startup_pools.conf[0].name
# to a non-default name while setting startup_pools.conf[0].pool to the default
# pool ID will fail).
#
# The priority regarding the default pool configuration is as follows:
#
# +--------------+      +-------------+      +-----------------------------+
# | Runtime.conf |  >   | em-odp.conf |   >  | em_conf_t::default_pool_cfg |
# +--------------+      +-------------+      +-----------------------------+
#
# The default pool configuration specified in runtime configuration file
# overrides the one given in the default configuration file(em-odp.conf), which
# overrides the one passed as a parameter to em_init().
#
# Valid values for 'event_type' are ("stringified" versions of EM constants):
#	"EM_EVENT_TYPE_SW"
#	"EM_EVENT_TYPE_PACKET"
#	"EM_EVENT_TYPE_VECTOR"
#
# Note that not all fields are mandatory, most of them are optional. Refer
# em_pool_cfg_t and em_pool_create() for a more detailed explanation of each
# option field.
#
# Note also that these are pool specific configurations given directly to
# em_pool_create(name, pool, pool_cfg), thus overriding the global settings
# such as pool.align_offset, pool.user_area_size, and pool.pkt_headroom set
# above.
#
# The startup pools will be deleted during em_term().
#
#startup_pools: {
#	# Number of startup pools.
#	# The number of pool configs in the 'conf' below must match this number.
#	# This number must be within the range: [1, EM_CONFIG_POOLS - 1].
#	num = 1 # Mandatory
#
#	# Pool configurations
#	conf: ({
#		# Pool name.
#		# Using "default" overrides the default pool config given
#		# to em_init().
#		name = "default" # Optional
#
#		# Pool ID.
#		# Setting to 1 (EM_POOL_DEFAULT) overrides the default
#		# pool config given to em_init().
#		# The pool ID must be within the range [0, EM_CONFIG_POOLS].
#		# Note that setting 'pool' to 0 (EM_POOL_UNDEF) has the same
#		# effect as leaving this out, in which case, EM will decide the
#		# pool ID.
#		pool = 1 # Optional
#
#		# Pool configurations. Corresponds to em_pool_cfg_t.
#		pool_cfg: {
#			# Event type
#			event_type = "EM_EVENT_TYPE_SW" # Mandatory
#
#			# Alignment offset
#			# Optional, but when 'align_offset' is used, both
#			# 'in_use' and 'value' must be given. So either leave
#			# all out or give the full setting.
#			# 'in_use = true', 'value = 0' --> set explicitly to 0
#			align_offset: {
#				in_use = true
#				value = 0 # Must be power of 2
#			} # Optional
#
#			# Event user area
#			# Optional, but when 'user_area' is used, both 'in_use'
#			# and 'size' must be given. So either leave all out or
#			# give full setting.
#			# 'in_use = true', 'value = 0' --> set explicitly to 0
#			user_area: {
#				in_use = true
#				size = 0
#			} # Optional
#
#			# Valid only for EM_EVENT_TYPE_PACKET.
#			pkt: {
#				# Pool-specific packet minimum headroom
#				# Optional, but when headrrom is used, both
#				# 'in_use' and 'value' must be given. So either
#				# leave all out or give full setting.
#				# 'in_use = true', 'value = 0' --> no headroom
#				headroom: {
#					in_use = false
#					value = 0
#				}
#			} # Optional
#
#			# Number of subpools.
#			# The number of subpool settings in 'subpools' below
#			# must match this number. This number must be within
#			# the range [1, EM_MAX_SUBPOOLS].
#			num_subpools = 4 # Mandatory
#
#			# Subpool settings.
#			subpools: ({
#					size = 256 # Mandatory
#					num = 16384 # Mandatory
#					cache_size = 64 # Optional
#				},
#				{
#					size = 512 # Mandatory
#					num = 1024 # Mandatory
#					cache_size = 32 # Optional
#				},
#				{
#					size = 1024 # Mandatory
#					num = 1024 # Mandatory
#					cache_size = 16 # Optional
#				},
#				{
#					size = 2048 # Mandatory
#					num = 1024 # Mandatory
#					cache_size = 8 # Optional
#				})
#		}
#	})
#}
